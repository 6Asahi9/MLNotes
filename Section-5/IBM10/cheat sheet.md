Absolutely, darling! ðŸ˜½ I can rewrite all of that in **fresh, original words** while keeping the meaning intact. Hereâ€™s a clean, copyright-safe version for your cheat sheet:

---

### NLP Tokenization and PyTorch Cheat Sheet (Original Version)

| Package / Method               | Description                                                                                                              | Example                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **NLTK**                       | A Python library for natural language processing, useful for tasks like tokenizing text and other basic NLP operations.  | `python import nltk nltk.download("punkt") from nltk.tokenize import word_tokenize text = "Unicorns exist. I saw one yesterday." tokens = word_tokenize(text) print(tokens) `                                                                                                                                                                                                                                                                 |
| **spaCy**                      | Open-source NLP library offering tokenization, part-of-speech tagging, and word embeddings.                              | `python import spacy nlp = spacy.load("en_core_web_sm") text = "Unicorns exist. I saw one yesterday." doc = nlp(text) tokens = [token.text for token in doc] print("Tokens:", tokens) `                                                                                                                                                                                                                                                       |
| **BertTokenizer**              | A tokenizer based on the WordPiece method, splitting words into subwords for robust NLP handling.                        | `python from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") tokenizer.tokenize("IBM taught me tokenization.") `                                                                                                                                                                                                                                                                             |
| **XLNetTokenizer**             | Uses Unigram and SentencePiece algorithms for subword tokenization.                                                      | `python from transformers import XLNetTokenizer tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased") tokenizer.tokenize("IBM taught me tokenization.") `                                                                                                                                                                                                                                                                            |
| **torchtext**                  | PyTorch library module for NLP, providing tools for tokenization, vocabulary building, and converting tokens to indices. | `python from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator dataset = [(1, "Intro to NLP"), (2, "Basics of PyTorch")] tokenizer = get_tokenizer("basic_english") def yield_tokens(data_iter): for _, text in data_iter: yield tokenizer(text) vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=["<unk>"]) vocab.set_default_index(vocab["<unk>"]) print(vocab.get_stoi()) ` |
| **vocab**                      | Maps words or tokens to numerical indices for model input.                                                               | `python def get_tokenized_sentence_and_indices(iterator): sentence = next(iterator) indices = [vocab[token] for token in sentence] return sentence, indices sentence, indices = get_tokenized_sentence_and_indices(my_iterator) print("Sentence:", sentence) print("Indices:", indices) `                                                                                                                                                     |
| **Special tokens**             | `<bos>` and `<eos>` mark the beginning and end of sequences; `<pad>` ensures uniform sequence lengths.                   | `python tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm') tokens = [] for line in lines: t = ['<bos>'] + tokenizer_en(line) + ['<eos>'] tokens.append(t) max_len = max(len(t) for t in tokens) for i in range(len(tokens)): tokens[i] += ['<pad>'] * (max_len - len(tokens[i])) `                                                                                                                                              |
| **Dataset class (PyTorch)**    | Custom datasets allow structured access to individual samples.                                                           | `python from torch.utils.data import Dataset class CustomDataset(Dataset): def __init__(self, sentences): self.sentences = sentences def __len__(self): return len(self.sentences) def __getitem__(self, idx): return self.sentences[idx] dataset = CustomDataset(["Hello world.", "NLP is fun!"]) print(dataset[0]) `                                                                                                                        |
| **DataLoader class (PyTorch)** | Facilitates efficient batch loading and iteration over datasets.                                                         | `python from torch.utils.data import DataLoader dataloader = DataLoader(dataset, batch_size=2, shuffle=True) for batch in dataloader: print(batch) `                                                                                                                                                                                                                                                                                          |
| **Custom collate function**    | User-defined function to process, tokenize, map to indices, and pad batches before feeding them into a model.            | `python from torch.nn.utils.rnn import pad_sequence def collate_fn(batch): tensor_batch = [] for sample in batch: tokens = tokenizer(sample) tensor_batch.append(torch.tensor([vocab[token] for token in tokens])) padded = pad_sequence(tensor_batch, batch_first=True) return padded dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn) `                                                                  |

---

If you want, darling, I can also make a **cute Miya-themed version** of this cheat sheet so itâ€™s fun to read while studying ðŸ˜½âœ¨.

Do you want me to do that?
