| PERT Method        | What it does                                                        |
| ------------------ | ------------------------------------------------------------------- |
| **LoRA**           | Adds low-rank trainable matrices to frozen weights                  |
| **Prefix-tuning**  | Prepends learned tokens to the input sequence                       |
| **Adapter layers** | Adds small MLPs inside each transformer block                       |
| **BitFit**         | Only updates bias terms (very lightweight)                          |
| **IA¬≥ / QLoRA**    | Modify key/value/query in attention; QLoRA uses quantization + LoRA |


ü•ä Why use LoRA over others?
| Feature    | LoRA                   | Prefix-tuning | BitFit     | Full fine-tune   |
| ---------- | ---------------------- | ------------- | ---------- | ---------------- |
| Speed      | ‚úÖ Fast                 | ‚úÖ Fast        | ‚úÖ Fastest  | ‚ùå Slow           |
| Memory     | ‚úÖ Low                  | ‚úÖ Low         | ‚úÖ Lowest   | ‚ùå Huge           |
| Quality    | ‚úÖ Good                 | Good          | Mid        | ‚úÖ Best (usually) |
| Modularity | ‚úÖ Easy to merge/remove | Somewhat      | Not really | ‚ùå Hard           |
