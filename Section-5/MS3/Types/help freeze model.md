| PERT Method        | What it does                                                        |
| ------------------ | ------------------------------------------------------------------- |
| **LoRA**           | Adds low-rank trainable matrices to frozen weights                  |
| **Prefix-tuning**  | Prepends learned tokens to the input sequence                       |
| **Adapter layers** | Adds small MLPs inside each transformer block                       |
| **BitFit**         | Only updates bias terms (very lightweight)                          |
| **IA¬≥ / QLoRA**    | Modify key/value/query in attention; QLoRA uses quantization + LoRA |


ü•ä Why use LoRA over others?
| Feature    | LoRA                   | Prefix-tuning | BitFit     | Full fine-tune   |
| ---------- | ---------------------- | ------------- | ---------- | ---------------- |
| Speed      | ‚úÖ Fast                 | ‚úÖ Fast        | ‚úÖ Fastest  | ‚ùå Slow           |
| Memory     | ‚úÖ Low                  | ‚úÖ Low         | ‚úÖ Lowest   | ‚ùå Huge           |
| Quality    | ‚úÖ Good                 | Good          | Mid        | ‚úÖ Best (usually) |
| Modularity | ‚úÖ Easy to merge/remove | Somewhat      | Not really | ‚ùå Hard           |

after freezing, change what?
| Module                                     | Why it's important                                                                                 |
| ------------------------------------------ | -------------------------------------------------------------------------------------------------- |
| `query`, `key`, `value` (inside attention) | This is where the model learns *what to focus on*. Massive impact. Tiny modules. Perfect for LoRA. |
| `attention_output`                         | Projects that focus outward ‚Äî also useful, but depends on the model.                               |
| `ffn` (feed-forward nets)                  | Sometimes targeted, especially in newer LoRA variants.                                             |
| `layer_norm`, `softmax` etc                | Rarely (if ever) LoRA'd ‚Äî they don‚Äôt *learn*, they *normalize or finalize*.                        |

| Word     | What I‚Äôm looking for (Query) | What each word represents (Key) | Actual meaning to bring (Value)      |
| -------- | ---------------------------- | ------------------------------- | ------------------------------------ |
| "cat"    | Looking for cute animal info | "cat": small fuzzy hunter       | "cat": soft, sits in lap, purrs      |
| "barked" | Looking for sound info       | "dog": loud, rough              | "barked": sharp sound                |
| "Miya"   | Looking for royalty info     | "Miya": majestic goddess        | "Miya": fluff queen, lives rent free |
