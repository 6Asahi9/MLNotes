Ahhh, darling üòΩüíñ I can totally rewrite that in **my own words** for you, keeping it ADHD-friendly and Miya-approved, so it‚Äôs not copyrighted:

---

## **Cheat Sheet: Unsupervised Learning Models**

### **Dimensionality Reduction Methods**

**1Ô∏è‚É£ PCA (Principal Component Analysis)**

* **What it does:** Reduces data dimensions linearly by keeping the directions with most variance.
* **Pros:** Easy to understand, removes noise.
* **Cons:** Can miss nonlinear patterns.
* **Uses:** Feature extraction, compressing data.
* **Key parameters:**

  * `n_components` ‚Üí how many dimensions to keep
  * `whiten` ‚Üí scale components if needed
* **Code example:**

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
```

---

**2Ô∏è‚É£ t-SNE**

* **What it does:** Nonlinear dimensionality reduction, emphasizes **local clusters**.
* **Pros:** Great for visualizing complex, high-dimensional data.
* **Cons:** Slow on large datasets, distances between clusters aren‚Äôt reliable.
* **Uses:** Visualization, anomaly detection.
* **Key parameters:**

  * `n_components` ‚Üí output dimensions (usually 2 or 3)
  * `perplexity` ‚Üí balances local/global attention
  * `learning_rate` ‚Üí step size during optimization
* **Code example:**

```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)
```

---

**3Ô∏è‚É£ UMAP (Uniform Manifold Approximation and Projection)**

* **What it does:** Nonlinear reduction that preserves **both local and global structure**.
* **Pros:** Fast, works well on large datasets.
* **Cons:** Sensitive to parameters.
* **Uses:** Visualization, feature extraction.
* **Key parameters:**

  * `n_neighbors` ‚Üí size of local neighborhood
  * `min_dist` ‚Üí how close points can be in reduced space
  * `n_components` ‚Üí number of dimensions to keep
* **Code example:**

```python
from umap.umap_ import UMAP
umap = UMAP(n_neighbors=15, min_dist=0.1, n_components=2)
```

---

### **Clustering Algorithms**

**1Ô∏è‚É£ K-Means**

* **How it works:** Groups points into `k` clusters based on centroids.
* **Pros:** Fast, simple.
* **Cons:** Sensitive to starting centroids.
* **Uses:** Customer segmentation, pattern recognition.
* **Key parameters:**

  * `n_clusters` ‚Üí number of clusters
  * `init` ‚Üí method for initializing centroids
  * `n_init` ‚Üí times to repeat with different seeds
* **Code example:**

```python
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
```

**2Ô∏è‚É£ DBSCAN**

* **How it works:** Density-based clustering, detects outliers automatically.
* **Pros:** No need to specify number of clusters.
* **Cons:** Struggles with clusters of varying density.
* **Uses:** Anomaly detection, spatial data clustering.
* **Key parameters:**

  * `eps` ‚Üí max distance to consider neighbors
  * `min_samples` ‚Üí minimum points to form a cluster
* **Code example:**

```python
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
```

**3Ô∏è‚É£ HDBSCAN**

* **How it works:** Hierarchical version of DBSCAN, better for varying densities.
* **Pros:** Handles complex clusters well.
* **Cons:** Slower than DBSCAN.
* **Uses:** Large datasets, complex clustering problems.
* **Key parameters:**

  * `min_cluster_size` ‚Üí smallest cluster size
  * `min_samples` ‚Üí minimum samples for a cluster
* **Code example:**

```python
import hdbscan
clusterer = hdbscan.HDBSCAN(min_cluster_size=5)
```

---

### **Associated Functions / Utilities**

* **make\_blobs** ‚Üí create synthetic Gaussian clusters

```python
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=100, centers=2, random_state=42)
```

* **multivariate\_normal** ‚Üí generate samples from a multivariate normal distribution

```python
from numpy.random import multivariate_normal
samples = multivariate_normal(mean=[0,0], cov=[[1,0],[0,1]], size=100)
```

* **Plotly 3D Scatter** ‚Üí for visualizing 3D data

```python
import plotly.express as px
fig = px.scatter_3d(df, x='x', y='y', z='z')
fig.show()
```

* **GeoPandas / Contextily** ‚Üí spatial data handling and basemaps

```python
import geopandas as gpd
import contextily as ctx

gdf = gpd.GeoDataFrame(df, geometry='geometry')
gdf = gdf.to_crs(epsg=3857)
ax = gdf.plot(figsize=(10,10))
ctx.add_basemap(ax)
```

* **Explained variance in PCA** ‚Üí check how much variance each component keeps

```python
pca.fit(X)
variance_ratio = pca.explained_variance_ratio_
```

---

If you want, I can **also make this into a super ADHD/Miya-style visual cheat sheet** with little icons and ‚Äúpaw-approved tips‚Äù for each method so you can glance and instantly know when to use PCA vs UMAP vs t-SNE vs clustering.

Do you want me to do that? üêæ‚ú®
