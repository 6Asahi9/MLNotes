ğŸ”¢ 1. Decision Trees
Split data with yes/no questions

Build a tree to make decisions

ğŸ“Œ Classification & regression

ğŸ§® 2. Principal Component Analysis (PCA)
Reduces number of features

Keeps most important info

ğŸ“Œ Dimensionality reduction

ğŸ“ 3. Support Vector Machines (SVM)
Finds best dividing line (or hyperplane)

Maximizes margin between classes

ğŸ“Œ Classification

ğŸ£ 4. Naive Bayes
Uses Bayesâ€™ Theorem

Assumes features are independent

ğŸ“Œ Text classification, spam detection

ğŸ¯ 5. K-Means Clustering
Groups data into K clusters

Repeats: assign â†’ update centers

ğŸ“Œ Unsupervised clustering

ğŸ§± 6. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
Finds clusters based on density

Labels outliers as noise

ğŸ“Œ Irregular shapes, noisy data

ğŸ›’ 7. Apriori Algorithm
Finds frequent item combinations

Builds association rules

ğŸ“Œ Market basket analysis

ğŸŒˆ 8. t-SNE (t-distributed Stochastic Neighbor Embedding)
Reduces dimensions to 2D/3D

Preserves local structure

ğŸ“Œ Data visualization

ğŸŒ² 9. Isolation Forest
Anomaly detection by isolating points

Outliers are easier to isolate

ğŸ“Œ Fraud detection, rare events

ğŸŒ³ 10. Hierarchical Clustering
Builds a cluster tree (dendrogram)

Zoom in/out to choose cluster detail

ğŸ“Œ When you donâ€™t know K or want nested groups
