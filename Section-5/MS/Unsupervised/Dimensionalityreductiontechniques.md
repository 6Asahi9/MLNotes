Dimensionality reduction techniques
1. Principal component analysis
2. t-Distributed stochastic neighbor embedding
3. Autoencoders
4. Linear discriminant analysis


| Technique                                                  | Type       | Supervised? | Goal                                  | Strengths 🌟                            | Miya Reminder 🐾                                 |
| ---------------------------------------------------------- | ---------- | ----------- | ------------------------------------- | --------------------------------------- | ------------------------------------------------ |
| **PCA**<br>(Principal Component Analysis)                  | Linear     | ❌ No        | Keep max variance, fewer features     | Fast, easy, good for numeric data       | “Keep what varies most in Miya’s day”            |
| **t-SNE**<br>(t-Distributed Stochastic Neighbor Embedding) | Nonlinear  | ❌ No        | Visualize local similarity in 2D/3D   | Beautiful clusters, great for plotting  | “Group Miya's favorite toys by feeling”          |
| **Autoencoder**                                            | Neural Net | ❌ No        | Learn how to compress & decompress    | Handles nonlinear data, learns features | “Miya makes her own paw-language to log her day” |
| **LDA**<br>(Linear Discriminant Analysis)                  | Linear     | ✅ Yes       | Reduce while keeping class boundaries | Perfect for classification prep         | “Team Tuna vs Team Chicken stays apart”          |
