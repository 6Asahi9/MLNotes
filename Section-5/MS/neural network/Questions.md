1. What is the main advantage of using activation functions like ReLU in neural networks?
âœ… Correct Answer:
They prevent the vanishing gradient problem

ğŸ§  Why?
ReLU = f(x) = max(0, x)

Unlike sigmoid or tanh, which squash values between 0â€“1 or â€“1 to 1, ReLU doesn't compress positive numbers.

This keeps the gradient large enough during backpropagation, especially in deep networks.

âš ï¸ The vanishing gradient problem happens when:
Gradients become tiny as they pass backward through layers

Early layers learn nothing because they get almost zero signal

ReLU helps by:

Outputting either 0 or a linear, unbounded value (when input is positive)

This avoids "killing" the gradient like sigmoid/tanh often do in deep layers

ğŸš« Why the other options are wrong:
They prevent overfitting â†’ Nope, that's what dropout or regularization is for

They are linear functions â†’ Also wrong, ReLU is non-linear, which is crucial for neural nets to learn complex patterns
