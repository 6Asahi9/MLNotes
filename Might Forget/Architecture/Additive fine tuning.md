Ahh yes, Asahi, now weâ€™re getting into the **nuances**, and youâ€™re right to be skeptical. ğŸ˜¼

Letâ€™s break this carefully:

---

### **Soft Prompts / Prefix Tuning vs Additive Fine-Tuning**

1. **Soft prompts / prefix tuning:**

   * Adds **learnable vectors** to the input of **attention layers**.
   * The **original model weights stay frozen**.
   * It **doesnâ€™t add new layers with activations**, it just **prepends something to the queries/keys** in attention.
   * âœ… Subtle: affects attention, but in a â€œvector prependingâ€ way, **not a whole new layer with parameters like weights and biases**.

2. **Additive fine-tuning:**

   * Literally adds **new task-specific layers (like MLPs or small residual blocks)** into the model architecture.
   * The **original transformer layers are frozen**.
   * This matches Priyaâ€™s description: *â€œadds task-specific layers between attention blocksâ€*.

---

### **Why not Soft Prompts here?**

* Soft prompts only **prepend vectors at attention input**, they **donâ€™t insert entire new layers**.
* Priya is **inserting new layers**, which is literally **additive fine-tuning**.

---

### **TL;DR:**

* **Prefix/soft prompt tuning** â†’ tweaks input to attention.
* **Additive fine-tuning** â†’ adds entirely **new layers** into the transformer.

So, despite the similarity (both are PEFT, both freeze original weights), **Priyaâ€™s scenario clearly describes additive fine-tuning**.

---

If you want, I can make a **tiny â€œMiya-approved diagramâ€ showing soft prompts vs additive fine-tuning vs LoRA inside a transformer**, so itâ€™s visually impossible to confuse. ğŸ¾âœ¨

Do you want me to do that?
