1️⃣ Fourier Transformation (FT)

What it does: It converts a signal from the time domain (how a signal changes over time) to the frequency domain (which frequencies are present in that signal).

Example: If you have a sound wave, FT tells you which notes or frequencies are in it.

Why useful: For audio, vibrations, or any periodic signal, it helps us see hidden patterns in frequency rather than just in time.

Think of it like taking a chord from a piano and figuring out which individual notes are being played.

2️⃣ TF-IDF (Term Frequency – Inverse Document Frequency)

What it does: Measures how important a word is in a document compared to a collection of documents (a corpus).

Components:

TF (Term Frequency): How often the word appears in a document.

IDF (Inverse Document Frequency): Reduces the weight of words that appear in almost every document (like “the”, “and”).

Why useful: Helps NLP models focus on informative words rather than common filler words.

Imagine Miya reading a cat manual. "Meow" might appear everywhere, but "purring" is more special for one section—TF-IDF would highlight “purring.”

3️⃣ Spectrogram Analysis

What it does: Visualizes how a signal’s frequency content changes over time.

Difference from FT: FT gives all frequencies at once (loses time info), while a spectrogram shows a time × frequency heatmap.

Why useful: For audio, it helps detect patterns like speech, music notes, or even cat purrs over time.

Think of it as a color map showing which frequencies “light up” at which times.
