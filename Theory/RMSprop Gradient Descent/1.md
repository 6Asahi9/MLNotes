![](/images/image_2025-03-03_212425511.png)

Let‚Äôs break it down step by step:

### 1Ô∏è‚É£ In normal gradient descent:

* We compute the gradient ùëît to see how much to change Œ∏.

No fancy tricks, just move in the opposite direction of the gradient.

### 2Ô∏è‚É£ Why square in RMSprop?

* We take gt^2 to track the magnitude of gradients over time.

This helps identify large and small updates.

### 3Ô∏è‚É£ Why take the square root?

* If we only used squared values, the updates might become too tiny over time. Taking root st balances things out so we don‚Äôt shrink updates too much.

### 4Ô∏è‚É£ Why add œµ?

it is a tiny constant added to prevent division by zero. since if we divide alpha by 0 cos of an accident , it will error and œµ is preventing it

* To avoid division by zero and keep things stable. So, yes, RMSprop:

* Focuses more on recent gradients (like Momentum).
* Smooths out learning by adapting step sizes
