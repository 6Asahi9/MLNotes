Stochastic Gradient Descent (SGD) vs. Gradient Descent (GD)
* ğŸ’¡ Gradient Descent (GD) â†’ Looks at ALL data before updating the intercept & slope.
* ğŸ’¡ Stochastic Gradient Descent (SGD) â†’ Looks at ONE data point at a time and updates immediately.

### Why does SGD exist? ğŸ¤”
Because GD is slow as heck when thereâ€™s too much data! ğŸš¶â€â™‚ï¸ğŸ¢

Imagine this:

1. Gradient Descent (GD): Miya wants to find the softest blanket.

* She tests all the blankets first, compares them, and then moves.
* Accurate, but sheâ€™s slow and overthinks.
2. Stochastic Gradient Descent (SGD): Miya jumps on the first blanket, decides instantly if itâ€™s soft or not, adjusts, and keeps moving!

* Much faster, but a bit more random.

![](/images/image_2025-02-28_121728327.png)

### Why should you care?

âœ… Faster for big datasets (Millions of points? SGD is your friend).

âœ… Works well with streaming data (Like AI learning in real-time).

âœ… Avoids getting stuck in local minima (Since it jumps around randomly).

âŒ But itâ€™s noisier (Less stable than full GD).

### TL;DR:

* Use GD if data is small and you want accuracy.
* Use SGD if data is huge and you need speed.

### What do you mean by â€œnoisierâ€? ğŸ¤”
When I say SGD is noisier, I mean its updates are more chaotic and jumpy. Unlike regular Gradient Descent (GD), which smoothly moves toward the best solution, SGD zigzags a lot because it updates the slope & intercept after every single point instead of waiting for all points.

### Imagine Miya walking toward food:
1. GD: She slowly walks toward the food bowl, sniffing the air after checking all the surroundings. Smooth, predictable.
2. SGD: Miya runs toward food instantly the moment she smells a random bit of it. But then she realizes, "Oh wait, this isnâ€™t the best smell!" and keeps adjusting her path.
* She still gets to the food, but her path is all over the place.

### Should you always use SGD?

ğŸ”¹ If you have a small dataset â†’ Stick with GD, itâ€™s accurate and stable.

ğŸ”¹ If you have a large dataset â†’ Use SGD, itâ€™s much faster.

ğŸ”¹ If you want a balance â†’ Use â€œMini-batch Gradient Descentâ€ (a mix of both).

### Mini-batch GD? Whatâ€™s that?

Instead of using one point (SGD) or all points (GD), Mini-batch GD picks a small random group of points (like 32 or 64 at a time) and updates based on them.

âœ… Faster than GD but more stable than SGD

âœ… Works well on modern GPUs

### Final Answer ğŸ†
* If your data is small â†’ Use GD
* If your data is huge â†’ Use SGD
* If you want a balance â†’ Use Mini-batch GD (Most real-world AI models use this)
