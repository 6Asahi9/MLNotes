Stochastic Gradient Descent (SGD) vs. Gradient Descent (GD)
* 💡 Gradient Descent (GD) → Looks at ALL data before updating the intercept & slope.
* 💡 Stochastic Gradient Descent (SGD) → Looks at ONE data point at a time and updates immediately.

### Why does SGD exist? 🤔
Because GD is slow as heck when there’s too much data! 🚶‍♂️🐢

Imagine this:

1. Gradient Descent (GD): Miya wants to find the softest blanket.

* She tests all the blankets first, compares them, and then moves.
* Accurate, but she’s slow and overthinks.
2. Stochastic Gradient Descent (SGD): Miya jumps on the first blanket, decides instantly if it’s soft or not, adjusts, and keeps moving!

* Much faster, but a bit more random.

![](/images/image_2025-02-28_121728327.png)

### Why should you care?

✅ Faster for big datasets (Millions of points? SGD is your friend).

✅ Works well with streaming data (Like AI learning in real-time).

✅ Avoids getting stuck in local minima (Since it jumps around randomly).

❌ But it’s noisier (Less stable than full GD).

### TL;DR:

* Use GD if data is small and you want accuracy.
* Use SGD if data is huge and you need speed.

### What do you mean by “noisier”? 🤔
When I say SGD is noisier, I mean its updates are more chaotic and jumpy. Unlike regular Gradient Descent (GD), which smoothly moves toward the best solution, SGD zigzags a lot because it updates the slope & intercept after every single point instead of waiting for all points.

### Imagine Miya walking toward food:
1. GD: She slowly walks toward the food bowl, sniffing the air after checking all the surroundings. Smooth, predictable.
2. SGD: Miya runs toward food instantly the moment she smells a random bit of it. But then she realizes, "Oh wait, this isn’t the best smell!" and keeps adjusting her path.
* She still gets to the food, but her path is all over the place.

### Should you always use SGD?

🔹 If you have a small dataset → Stick with GD, it’s accurate and stable.

🔹 If you have a large dataset → Use SGD, it’s much faster.

🔹 If you want a balance → Use “Mini-batch Gradient Descent” (a mix of both).

### Mini-batch GD? What’s that?

Instead of using one point (SGD) or all points (GD), Mini-batch GD picks a small random group of points (like 32 or 64 at a time) and updates based on them.

✅ Faster than GD but more stable than SGD

✅ Works well on modern GPUs

### Final Answer 🏆
* If your data is small → Use GD
* If your data is huge → Use SGD
* If you want a balance → Use Mini-batch GD (Most real-world AI models use this)
