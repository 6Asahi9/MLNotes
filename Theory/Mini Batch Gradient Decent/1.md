Mini-Batch Gradient Descent (MBGD) is basically a middle ground between Batch Gradient Descent (which uses all data) and Stochastic Gradient Descent (which uses one sample at a time).

### How to Add Mini-Batch to Your Gradient Descent
Instead of using all data at once (Batch GD) or one sample at a time (SGD), we:

1. Divide the dataset into small batches (e.g., 32, 64, 128 samples per batch).
2. Compute gradients for each batch instead of the whole dataset.
3. Update the parameters after each batch, not after every single sample or the entire dataset.

### Algorithm for Mini-Batch GD

![](/images/image_2025-02-28_130534297.png)

OR

![](/images/image_2025-02-28_130651364.png)

### ðŸ”¹ Key Differences from Standard GD
âœ… Batch GD â†’ Uses all data points for each update (slow for large datasets)

âœ… SGD â†’ Uses one data point at a time (high variance, faster but noisier updates)

âœ… Mini-Batch GD â†’ Uses a small batch (balances stability & efficiency)

###ðŸ”¹ When to Use Mini-Batch GD?
1. When Batch GD is too slow for large datasets
2, When SGD is too noisy and unstable
3. When you want the best of both worlds (faster training but still stable)
