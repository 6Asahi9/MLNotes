### 1️⃣ Motivation for Derivatives in Neural Networks
Neural networks learn by adjusting their weights to minimize errors. But how do we know which way to adjust the weights?

The key idea: Use derivatives to find the direction and size of changes.
If a weight 
𝑊
𝑖
𝑗​
  increases the error, decrease it. If it decreases the error, increase it.

This process is done using gradient descent, which relies on derivatives of a function that measures error.

Thus, derivatives help optimize the network by guiding weight updates.

### 2️⃣ Cost Function (Measures How Bad the Network Is Performing)
The Cost Function (also called Loss Function) tells us how far off the network’s predictions are from the actual values.

It is a function that takes in predictions vs. actual values and returns a single number (the cost).

The goal of training is to minimize this cost by adjusting weights using derivatives.

![](/images/image_2025-03-15_121827166.png)
