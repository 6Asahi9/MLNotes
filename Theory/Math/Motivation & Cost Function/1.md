### 1ï¸âƒ£ Motivation for Derivatives in Neural Networks
Neural networks learn by adjusting their weights to minimize errors. But how do we know which way to adjust the weights?

The key idea: Use derivatives to find the direction and size of changes.
If a weight 
ğ‘Š
ğ‘–
ğ‘—â€‹
  increases the error, decrease it. If it decreases the error, increase it.

This process is done using gradient descent, which relies on derivatives of a function that measures error.

Thus, derivatives help optimize the network by guiding weight updates.

### 2ï¸âƒ£ Cost Function (Measures How Bad the Network Is Performing)
The Cost Function (also called Loss Function) tells us how far off the networkâ€™s predictions are from the actual values.

It is a function that takes in predictions vs. actual values and returns a single number (the cost).

The goal of training is to minimize this cost by adjusting weights using derivatives.

![](/images/image_2025-03-15_121827166.png)
